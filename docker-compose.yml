# Propwise Data Platform - Complete Stack
# ==========================================
# Architecture:
#   Source: MySQL (operational data)
#   Lake: MinIO + Iceberg
#   Query: Trino
#   DWH: PostgreSQL
#   Processing: Python + Pandas (current), PySpark (available for scale)
#   Quality: Great Expectations
#   Metadata: PostgreSQL
#   Orchestration: Airflow
#
# Note: PySpark available but not actively used for current data volumes
#       Current processing uses Python + Pandas for simplicity
# ==========================================

services:
  # ===========================================
  # SOURCE DATABASE - MySQL
  # ===========================================
  mysql:
    image: mysql:8.0
    container_name: propwise-mysql-source
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: root123
      MYSQL_DATABASE: propwise_source
      MYSQL_USER: source_user
      MYSQL_PASSWORD: source123
    volumes:
      - mysql_data:/var/lib/mysql
      - ./infrastructure/mysql/init:/docker-entrypoint-initdb.d
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-uroot", "-proot123"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - propwise-network

  # ===========================================
  # DATA LAKE - MinIO (S3-compatible)
  # ===========================================
  minio:
    image: minio/minio:latest
    container_name: propwise-minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - propwise-network

  # MinIO bucket initialization
  minio-init:
    image: minio/mc:latest
    container_name: propwise-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set propwise http://minio:9000 minioadmin minioadmin123;
      mc mb propwise/raw-data --ignore-existing;
      mc mb propwise/staging --ignore-existing;
      mc mb propwise/curated --ignore-existing;
      mc mb propwise/iceberg-warehouse --ignore-existing;
      mc mb propwise/rejected --ignore-existing;
      mc mb propwise/logs --ignore-existing;
      mc mb propwise/logs/ingestion --ignore-existing;
      mc mb propwise/logs/airflow --ignore-existing;
      mc mb propwise/logs/quality --ignore-existing;
      mc mb propwise/logs/trino --ignore-existing;
      echo 'All buckets created successfully';
      exit 0;
      "
    networks:
      - propwise-network

  # ===========================================
  # ICEBERG REST CATALOG
  # ===========================================
  iceberg-rest:
    image: tabulario/iceberg-rest:latest
    container_name: propwise-iceberg-rest
    ports:
      - "8181:8181"
    environment:
      AWS_ACCESS_KEY_ID: minioadmin
      AWS_SECRET_ACCESS_KEY: minioadmin123
      AWS_REGION: us-east-1
      CATALOG_WAREHOUSE: s3://iceberg-warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_PATH__STYLE__ACCESS: "true"
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - propwise-network

  # ===========================================
  # QUERY ENGINE - Trino
  # ===========================================
  trino:
    image: trinodb/trino:435
    container_name: propwise-trino
    ports:
      - "8090:8080"
    volumes:
      - ./infrastructure/trino/etc:/etc/trino
      - ./infrastructure/trino/catalog:/etc/trino/catalog
    depends_on:
      - postgres-dwh
      - minio
      - iceberg-rest
    networks:
      - propwise-network

  # ===========================================
  # DATA WAREHOUSE - PostgreSQL
  # ===========================================
  postgres-dwh:
    image: postgres:15
    container_name: propwise-postgres-dwh
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: dwh_user
      POSTGRES_PASSWORD: dwh123
      POSTGRES_DB: propwise_dwh
    volumes:
      - postgres_dwh_data:/var/lib/postgresql/data
      - ./infrastructure/postgres-dwh/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dwh_user -d propwise_dwh"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - propwise-network

  # ===========================================
  # METADATA DATABASE - PostgreSQL
  # ===========================================
  postgres-metadata:
    image: postgres:15
    container_name: propwise-postgres-metadata
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: metadata_user
      POSTGRES_PASSWORD: metadata123
      POSTGRES_DB: propwise_metadata
    volumes:
      - postgres_metadata_data:/var/lib/postgresql/data
      - ./infrastructure/postgres-metadata/init:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U metadata_user -d propwise_metadata"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - propwise-network

  # ===========================================
  # PROCESSING ENGINE - PySpark (Available, not actively used)
  # ===========================================
  spark-master:
    image: apache/spark:3.5.0
    container_name: propwise-spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_NO_DAEMONIZE=true
    command: /opt/spark/sbin/start-master.sh
    volumes:
      - ./infrastructure/spark/jars:/opt/spark/jars/custom
      - ./ingestion:/opt/spark/work/etl
      - ./processing:/opt/spark/work/processing
    networks:
      - propwise-network

  spark-worker-1:
    image: apache/spark:3.5.0
    container_name: propwise-spark-worker-1
    ports:
      - "8081:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    volumes:
      - ./infrastructure/spark/jars:/opt/spark/jars/custom
      - ./ingestion:/opt/spark/work/etl
      - ./processing:/opt/spark/work/processing
    depends_on:
      - spark-master
    networks:
      - propwise-network

  spark-worker-2:
    image: apache/spark:3.5.0
    container_name: propwise-spark-worker-2
    ports:
      - "8082:8081"
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_WEBUI_PORT=8081
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    command: /opt/spark/sbin/start-worker.sh spark://spark-master:7077
    volumes:
      - ./infrastructure/spark/jars:/opt/spark/jars/custom
      - ./ingestion:/opt/spark/work/etl
      - ./processing:/opt/spark/work/processing
    depends_on:
      - spark-master
    networks:
      - propwise-network

  # ===========================================
  # ORCHESTRATION - Apache Airflow
  # ===========================================
  airflow-init:
    image: apache/airflow:2.8.0-python3.11
    container_name: propwise-airflow-init
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://metadata_user:metadata123@postgres-metadata:5432/propwise_metadata
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-must-be-32-bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_DB_MIGRATE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin123
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./ingestion:/opt/airflow/ingestion
      - ./processing:/opt/airflow/processing
    entrypoint: /bin/bash
    command:
      - -c
      - |
        pip install pandas minio pyarrow --quiet
        airflow db migrate
        airflow users create --username admin --password admin123 --firstname Admin --lastname User --role Admin --email admin@propwise.com || true
        echo "Airflow initialized successfully"
    depends_on:
      postgres-metadata:
        condition: service_healthy
    networks:
      - propwise-network

  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: propwise-airflow-webserver
    ports:
      - "8085:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://metadata_user:metadata123@postgres-metadata:5432/propwise_metadata
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-must-be-32-bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./ingestion:/opt/airflow/ingestion
      - ./processing:/opt/airflow/processing
    command: >
      bash -c "pip install -r /opt/airflow/requirements.txt --quiet && airflow webserver"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres-metadata:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - propwise-network

  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: propwise-airflow-scheduler
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://metadata_user:metadata123@postgres-metadata:5432/propwise_metadata
      - AIRFLOW__CORE__FERNET_KEY=your-fernet-key-here-must-be-32-bytes
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./ingestion:/opt/airflow/ingestion
      - ./processing:/opt/airflow/processing
    command: >
      bash -c "pip install -r /opt/airflow/requirements.txt --quiet && airflow scheduler"
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres-metadata:
        condition: service_healthy
    networks:
      - propwise-network

  # ===========================================
  # DATA QUALITY - Great Expectations
  # ===========================================
  great-expectations:
    image: python:3.11-slim
    container_name: propwise-great-expectations
    working_dir: /app
    volumes:
      - ./great_expectations:/app/great_expectations
      - ./ingestion:/app/etl
    environment:
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin123
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USER=source_user
      - MYSQL_PASSWORD=source123
      - MYSQL_DATABASE=propwise_source
      - DWH_HOST=postgres-dwh
      - DWH_PORT=5432
      - DWH_USER=dwh_user
      - DWH_PASSWORD=dwh123
      - DWH_DATABASE=propwise_dwh
    command: >
      /bin/bash -c "
        pip install great-expectations pandas sqlalchemy psycopg2-binary pymysql minio pyarrow boto3 --quiet &&
        tail -f /dev/null
      "
    depends_on:
      - mysql
      - postgres-dwh
      - minio
    networks:
      - propwise-network

# ===========================================
# VOLUMES
# ===========================================
volumes:
  mysql_data:
    driver: local
  minio_data:
    driver: local
  postgres_dwh_data:
    driver: local
  postgres_metadata_data:
    driver: local

# ===========================================
# NETWORKS
# ===========================================
networks:
  propwise-network:
    driver: bridge
    name: propwise-network

